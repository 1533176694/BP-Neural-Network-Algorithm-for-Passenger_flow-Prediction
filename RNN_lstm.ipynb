{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: Xent Loss  0.5557508926896193\n",
      "F: Xent Loss  0.5759326267664994\n",
      "N: Xent Loss  0.6474466390346325\n"
     ]
    }
   ],
   "source": [
    "# compute the xent loss \n",
    "def ent(p):\n",
    "    if p>1 or p<0: return('Not Probability')\n",
    "    elif p==0 or p==1: return (0) \n",
    "    else: return -(p*np.log(p)+(1-p)*np.log(1-p))\n",
    "    \n",
    "# Data: (x4,x10)= four possibilities: Prob(y=1, x4, x10)\n",
    "# (1,1) -> p(y=1)=0.65\n",
    "# (0,1) -> p(y=1)=0.25\n",
    "# (1,0) -> p(y=1)=0.45\n",
    "# (0,0) -> p(y=1)=0.10 \n",
    "\n",
    "# The question is: does the machine has the capability to probe the details to this level? \n",
    "# If not capable of probing x10 dependence, then x10 is essentially a hidden variable \n",
    "#    whose influence needs to be summer over (marginalized), so we have\n",
    "#    p(y=1,x4) = sum_x10 p(y=1,x4,x10) = (0.65+0.45)/2 = 0.55 \n",
    "# If not capable of probing neither x4 or x10, we need to sum over both (as if both were hidden units)\n",
    "#    so we have p(y=1)= sum_x10,x4 p(y=1,x4,x10) = (0.65+0.25+0.40+0.10)/4 = 0.35 \n",
    "\n",
    "# Got both x10 and x4\n",
    "plist = [0.65,0.25,0.45,0.10]\n",
    "print (\"B: Xent Loss \", sum(ent(p) for p in plist)/len(plist))\n",
    "\n",
    "# Got only x4\n",
    "plist = [0.55, 0.175]\n",
    "print (\"F: Xent Loss \", sum(ent(p) for p in plist)/len(plist))\n",
    "\n",
    "# Got none\n",
    "plist = [0.35]\n",
    "print (\"N: Xent Loss \", sum(ent(p) for p in plist)/len(plist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "# Configuration on Global Variables\n",
    "# 1. RNN Cell Configuration (normally num_neuron = state_size, but not necessary)\n",
    "#num_neuron = 10\n",
    "\n",
    "# 2. Recurrent Steps (Unfolding)\n",
    "num_steps = 25 # number of truncated backprop steps ('n' in the discussion above)\n",
    "pos_1=3\n",
    "pos_2=40\n",
    "\n",
    "# 3. Data Input/State/Output \n",
    "#seq_size = 50000000\n",
    "seq_size = 200000000\n",
    "state_size = 40\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# 4. Training (Mini-Batch, LearningRate, Dropout, ...)\n",
    "batch_size = 400\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(seq_size, batch_size, num_steps): \n",
    "    X = np.random.choice([0,1], p=[0.5,0.5], size=seq_size)   \n",
    "    pos1=4\n",
    "    # ------ Set the position here ! ---------------------\n",
    "    pos2=20\n",
    "    plist=[0.65, 0.45, 0.25, 0.10]\n",
    "    p2=np.zeros((2, 2))\n",
    "    p2[1,1]=plist[0]\n",
    "    p2[1,0]=plist[1]\n",
    "    p2[0,1]=plist[2]\n",
    "    p2[0,0]=plist[3]\n",
    "\n",
    "    X = np.random.choice([0,1], p = [0.5, 0.5], size = seq_size)\n",
    "    Prob = p2[np.roll(X,pos1), np.roll(X,pos2)]\n",
    "    Y = (np.random.rand(len(X)) < Prob).astype(int) \n",
    "    \n",
    "    epoch_size = seq_size // (batch_size*num_steps)\n",
    "    \n",
    "    X.resize(batch_size, epoch_size, num_steps)\n",
    "    Y.resize(batch_size, epoch_size, num_steps)\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        yield (X[:,i,:],Y[:,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#init_state = tf.zeros(tf.float32,[batch_size, state_size])\n",
    "#rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "#init_state = tf.zeros([batch_size, state_size])\n",
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "#rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state, dtype=\"float32\")\n",
    "\n",
    "#rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-85c96be39ad5>:11: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x111fcf208>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-5-85c96be39ad5>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "x, y, init_state (400, 25) (400, 25) (400, 80)\n",
      "run_outputs (400, 25, 40)\n",
      "final_state (400, 80)\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "#init_state=tf.placeholder(tf.float32, [batch_size, state_size+state_size])\n",
    "init_state = tf.zeros([batch_size, state_size*2])\n",
    "\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "#rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state, dtype=\"float32\")\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=False)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state, dtype='float32')\n",
    "\n",
    "print(\"x, y, init_state\", x.get_shape(), y.get_shape(), init_state.get_shape())\n",
    "\n",
    "print(\"run_outputs\", rnn_outputs.get_shape())\n",
    "print(\"final_state\", final_state.get_shape())\n",
    "\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes])\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step & Loss:  200 0.6467878025770187\n",
      "Step & Loss:  400 0.6354857847094536\n",
      "Step & Loss:  600 0.633861452639103\n",
      "Step & Loss:  800 0.6325184607505798\n",
      "Step & Loss:  1000 0.6295275154709816\n",
      "Step & Loss:  1200 0.6240663433074951\n",
      "Step & Loss:  1400 0.6206122502684593\n",
      "Step & Loss:  1600 0.614347128868103\n",
      "Step & Loss:  1800 0.6068462041020394\n",
      "Step & Loss:  2000 0.6015508890151977\n",
      "Step & Loss:  2200 0.5919282990694046\n",
      "Step & Loss:  2400 0.5746005138754845\n",
      "Step & Loss:  2600 0.5744057410955429\n",
      "Step & Loss:  2800 0.5743591180443763\n",
      "Step & Loss:  3000 0.5740392872691155\n",
      "Step & Loss:  3200 0.5731964427232742\n",
      "Step & Loss:  3400 0.5734378260374069\n",
      "Step & Loss:  3600 0.5730319094657897\n",
      "Step & Loss:  3800 0.5737210628390312\n",
      "Step & Loss:  4000 0.5734602302312851\n",
      "Step & Loss:  4200 0.5725761610269546\n",
      "Step & Loss:  4400 0.5726654013991356\n",
      "Step & Loss:  4600 0.5716512781381607\n",
      "Step & Loss:  4800 0.571621362566948\n",
      "Step & Loss:  5000 0.572330881357193\n",
      "Step & Loss:  5200 0.5715770941972732\n",
      "Step & Loss:  5400 0.5710314065217972\n",
      "Step & Loss:  5600 0.5714369428157806\n",
      "Step & Loss:  5800 0.5720272126793862\n",
      "Step & Loss:  6000 0.571290050148964\n",
      "Step & Loss:  6200 0.571413286626339\n",
      "Step & Loss:  6400 0.5708004632592201\n",
      "Step & Loss:  6600 0.5711202189326287\n",
      "Step & Loss:  6800 0.571264215707779\n",
      "Step & Loss:  7000 0.5710560974478721\n",
      "Step & Loss:  7200 0.5708888188004494\n",
      "Step & Loss:  7400 0.5714740645885468\n",
      "Step & Loss:  7600 0.5713414838910102\n",
      "Step & Loss:  7800 0.5712996861338615\n",
      "Step & Loss:  8000 0.5710413807630539\n",
      "Step & Loss:  8200 0.5707416754961013\n",
      "Step & Loss:  8400 0.5713267040252685\n",
      "Step & Loss:  8600 0.5705414739251137\n",
      "Step & Loss:  8800 0.5710050532221794\n",
      "Step & Loss:  9000 0.5714914360642434\n",
      "Step & Loss:  9200 0.5710697484016418\n",
      "Step & Loss:  9400 0.5717099139094353\n",
      "Step & Loss:  9600 0.5713779094815254\n"
     ]
    }
   ],
   "source": [
    "# Training the network \n",
    "def train_network(seq_size, num_steps, state_size, printout=10):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "               \n",
    "        training_loss = 0\n",
    "        training_state = np.zeros([batch_size, state_size*2])\n",
    "        \n",
    "        step = 0\n",
    "\n",
    "        for (X, Y) in gen_batch(seq_size, batch_size, num_steps):\n",
    "            training_loss_, training_state, _ = \\\n",
    "                sess.run([total_loss, final_state, train_step],\n",
    "                              feed_dict={x:X, y:Y, init_state:training_state})\n",
    "            training_loss += training_loss_\n",
    "            if step % printout == 0 and step > 0:\n",
    "                print(\"Step & Loss: \", step, training_loss/printout)\n",
    "                training_losses.append(training_loss/printout)\n",
    "                training_loss = 0\n",
    "            step +=1\n",
    "                \n",
    "    return training_losses\n",
    "\n",
    "training_losses = train_network(seq_size,num_steps,state_size, printout=200)\n",
    "print('Num of Unfolding (num_steps) ', num_steps)\n",
    "print('Num of Neurons in RNN Cell (state_size) ', state_size)\n",
    "print('Sequence Length(seq_size) ', seq_size)\n",
    "print('Batch,Steps,Epoch ', batch_size, num_steps, seq_size//(batch_size*num_steps))\n",
    "plt.ylim(0.45, 0.68)\n",
    "plt.plot(training_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
